# Sistema RAG con Embeddings y Bases Vectoriales

Sistema avanzado de recuperaciÃ³n de informaciÃ³n que implementa RAG (Retrieval-Augmented Generation) utilizando embeddings y bases de datos vectoriales para consultas inteligentes sobre documentos.

## ğŸ” Â¿QuÃ© hace este sistema?

Este proyecto implementa un sistema RAG completo que:

1. **Procesa documentos extensos** (PDF, TXT, MD, JSON)
2. **Genera embeddings vectoriales** de cada fragmento utilizando modelos de Hugging Face
3. **Almacena los vectores** en una base de datos vectorial (Qdrant)
4. **Permite consultas en lenguaje natural** sobre el contenido de los documentos
5. **Genera respuestas precisas** utilizando GPT-4o-mini con contexto relevante
6. **Muestra las fuentes de informaciÃ³n** utilizadas en cada respuesta
7. **Registra el uso de tokens y costos** de cada consulta a OpenAI

Para entender en profundidad cÃ³mo funciona este sistema, consulta: [ExplicaciÃ³n Conceptual: TecnologÃ­a RAG](docs/RAG-Concepto-Explicacion.md)

## âœ¨ CaracterÃ­sticas principales

- âœ… **Procesamiento inteligente de documentos** - DivisiÃ³n en fragmentos con solapamiento configurable
- âœ… **GeneraciÃ³n local de embeddings** - VectorizaciÃ³n con Hugging Face sin dependencias externas
- âœ… **Base de datos vectorial optimizada** - BÃºsqueda semÃ¡ntica eficiente con Qdrant
- âœ… **Interfaz de usuario intuitiva** - Frontend sencillo para gestionar documentos y consultas
- âœ… **GeneraciÃ³n aumentada con GPT** - Respuestas contextualizadas basadas en el contenido recuperado
- âœ… **Trazabilidad completa** - VisualizaciÃ³n de las fuentes usadas en cada respuesta
- âœ… **Monitoreo de costos** - Transparencia en el uso de tokens y costos estimados por consulta

## ğŸ“‹ Requisitos

- Docker y Docker Compose
- Clave API de OpenAI (solo para generaciÃ³n de respuestas)
- MÃ­nimo 4GB de RAM disponible

## ğŸš€ InstalaciÃ³n

1. **Clonar el repositorio**

```bash
git clone https://github.com/drosell271/rag-embeddings-system.git
cd rag-embeddings-system
```

2. **Configurar variables de entorno**

```bash
cp .env.example .env
```

Edita el archivo `.env` para incluir tu clave API de OpenAI:

```
OPENAI_API_KEY=tu_api_key_aqui
OPENAI_ORG_ID=tu_org_id_aqui
```

3. **Construir y ejecutar los contenedores**

```bash
docker-compose up -d
```

4. **Acceder a la aplicaciÃ³n**

Abre tu navegador y accede a http://localhost:3000

## ğŸ”„ Flujo de trabajo del sistema

El sistema implementa tres fases principales:

### 1. Procesamiento de documentos
```
Documento â†’ ExtracciÃ³n â†’ FragmentaciÃ³n â†’ VectorizaciÃ³n â†’ Almacenamiento
```

### 2. Consulta de informaciÃ³n
```
Pregunta â†’ VectorizaciÃ³n â†’ BÃºsqueda semÃ¡ntica â†’ RecuperaciÃ³n de fragmentos
```

### 3. GeneraciÃ³n de respuestas
```
Fragmentos relevantes + Pregunta â†’ GPT-4o-mini â†’ Respuesta contextualizada + MÃ©tricas de uso
```

## ğŸ’» GuÃ­a rÃ¡pida de uso

### Subir documentos

1. Haz clic en "Subir documento"
2. Introduce un tÃ­tulo descriptivo
3. Selecciona un archivo (PDF, TXT, MD o JSON)
4. Haz clic en "Procesar documento"

### Consultar informaciÃ³n

1. Formula tu pregunta en lenguaje natural
2. Haz clic en "Enviar consulta"
3. Revisa la respuesta, las fuentes utilizadas y el uso de tokens/costos

Para instrucciones detalladas, consulta el [Manual de Usuario](docs/Manual-Usuario.md).

## ğŸ“‚ Estructura del proyecto

```
rag-embeddings-system/
â”œâ”€â”€ docker-compose.yml     # ConfiguraciÃ³n de Docker
â”œâ”€â”€ Dockerfile             # ConfiguraciÃ³n de la imagen
â”œâ”€â”€ .env                   # Variables de entorno
â”œâ”€â”€ server.js              # Punto de entrada del servidor
â”œâ”€â”€ services/              # Servicios principales
â”‚   â”œâ”€â”€ documentService.js    # Procesamiento de documentos
â”‚   â”œâ”€â”€ huggingfaceService.js # GeneraciÃ³n de embeddings
â”‚   â”œâ”€â”€ openaiService.js      # GeneraciÃ³n de respuestas
â”‚   â”œâ”€â”€ qdrantService.js      # Base de datos vectorial
â”‚   â””â”€â”€ queryService.js       # Procesamiento de consultas
â”œâ”€â”€ routes/                # API endpoints
â”œâ”€â”€ utils/                 # Utilidades del sistema
â””â”€â”€ public/                # Interfaz de usuario
```

## ğŸ”§ ConfiguraciÃ³n avanzada

El sistema es altamente configurable a travÃ©s del archivo `.env`:

```
# Ajuste de fragmentaciÃ³n de documentos
CHUNK_SIZE=768             # TamaÃ±o de fragmentos (default: 512)
CHUNK_OVERLAP=100          # Solapamiento entre fragmentos (default: 50)

# Modelos disponibles
HUGGINGFACE_EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
OPENAI_COMPLETION_MODEL=gpt-4    # Afecta al costo por token
```

Para detalles tÃ©cnicos completos, consulta la [GuÃ­a TÃ©cnica](docs/Guia-Tecnica.md).

## âš ï¸ SoluciÃ³n de problemas comunes

### Error "ERR_REQUIRE_ESM"

Ocurre con la biblioteca `@xenova/transformers` porque es un mÃ³dulo ESM. SoluciÃ³n:

```javascript
// En lugar de:
const { pipeline } = require('@xenova/transformers');

// Usar:
const { pipeline } = await import('@xenova/transformers');
```

### Error de Qdrant sobre IDs invÃ¡lidos

Qdrant requiere UUIDs vÃ¡lidos o enteros como IDs. SoluciÃ³n:

```javascript
const { v4: uuidv4 } = require('uuid');
const pointId = uuidv4();
```

### Error "externally-managed-environment" en Python

Ocurre en Debian 12 al instalar paquetes con pip. SoluciÃ³n:

```dockerfile
RUN python3 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"
```

## ğŸ“„ Licencia

Este proyecto estÃ¡ licenciado bajo la licencia MIT - ver el archivo [LICENSE](LICENSE).

---

Desarrollado por [Daniel Rosell](https://github.com/drosell271)